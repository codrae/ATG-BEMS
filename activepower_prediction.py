# -*- coding: utf-8 -*-
"""Inha_activePower_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LCIVSmzKze-WC20BaYiQ85V2302Hg3NQ

#환경 구축
"""

# pyodbc 라이브러리 설치
# pyodbc는 Python에서 ODBC(Open Database Connectivity)를 통해 다양한 데이터베이스에 연결할 수 있도록 해주는 라이브러리입니다.
# 이 라이브러리는 Microsoft SQL Server, MySQL, PostgreSQL 등 여러 데이터베이스에 대한 ODBC 연결을 지원합니다.
# 설치 후, ODBC 드라이버를 사용하여 데이터베이스 연결과 쿼리 실행이 가능합니다.
!pip install pyodbc

# Commented out IPython magic to ensure Python compatibility.
# %%sh
# # unixodbc-dev 설치
# apt-get install -y unixodbc-dev
# 
# # Microsoft GPG 키를 다운로드하고 /etc/apt/trusted.gpg.d/에 저장
# curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor > /etc/apt/trusted.gpg.d/microsoft.gpg
# 
# # Ubuntu 버전에 맞는 Microsoft SQL Server 패키지 저장소 추가
# curl https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/prod.list > /etc/apt/sources.list.d/mssql-release.list
# 
# # 패키지 리스트 업데이트
# apt-get update
# 
# # Microsoft ODBC Driver 17 for SQL Server 설치
# ACCEPT_EULA=Y apt-get -q -y install msodbcsql17

"""#DB연결"""

# 데이터베이스 연결, 데이터 프레임 및 시각화 관련 라이브러리
import pyodbc
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# 데이터 전처리 및 성능 평가 관련 라이브러리
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# 머신러닝/딥러닝 관련 라이브러리 (PyTorch)
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, random_split, Subset

# 시스템, OS, 랜덤성 관련 설정 라이브러리
import os
import random
import datetime

# Google Colab 파일 다운로드 관련 라이브러리 (Google Colab 전용)
from google.colab import files

# 시각화에서 자주 사용되는 모듈
import matplotlib.pyplot as plt

# 설치된 ODBC 드라이버 목록을 가져와 리스트로 저장
# pyodbc.drivers()는 시스템에 설치된 모든 ODBC 드라이버의 이름을 반환합니다.
drivers = [driver for driver in pyodbc.drivers()]

# 사용 가능한 ODBC 드라이버 목록 출력
print(drivers)

# MSSQL 서버 연결 설정
server =   # MSSQL 서버 IP 주소
port =             # 서버 포트 번호
database =           # 데이터베이스 이름
username =       # MSSQL 서버 사용자 이름
password =     # MSSQL 서버 사용자 비밀번호
driver =   # 사용하는 ODBC 드라이버

# 연결 문자열 생성
# 서버 정보, 데이터베이스 이름, 사용자 인증 정보 등을 포함한 연결 문자열을 생성
conn_str = f'DRIVER={driver};SERVER={server},{port};DATABASE={database};UID={username};PWD={password}'

# pyodbc.connect()로 MSSQL 서버에 연결
connection = pyodbc.connect(conn_str)

# 커서 생성
cursor = connection.cursor()

# SQL 쿼리 실행
query  = "SELECT DataValue, DateTime FROM Tech_All_KWH WHERE Building =  ORDER BY DateTime"

# 쿼리 실행 후 결과 가져오기
cursor.execute(query)
rows = cursor.fetchall()  # 모든 행을 fetch

# 결과 중 상위 5개의 행을 출력
for i in range(5):
    print(rows[i])

# 최적화 측면에서의 추가 작업은 필요하지 않음.
# 데이터 가져오는 방식은 효율적이고, 반복문은 필요한 만큼만 사용되었음.

"""#데이터 로드 및 전처리"""

# 데이터 리스트를 pandas DataFrame으로 변환
# 'rows' 리스트의 각 요소가 리스트 형태로 변환된 후 DataFrame으로 만들어짐
# 각 열은 'DataValue', , 'DateTime'으로 정의
df = pd.DataFrame([list(row) for row in rows], columns=['DataValue', 'DateTime'])

# DateTime 컬럼을 datetime 형식으로 변환
# 이 단계에서는 'DateTime' 열을 pandas의 datetime 객체로 변환하여 날짜/시간 데이터로 처리 가능하게 함
df['DateTime'] = pd.to_datetime(df['DateTime'])

# DataFrame의 처음 5개 행을 출력하여 결과 확인
df.head()

# 함수로 정의
def analyze_dataframe(df):
    import pandas as pd
    import matplotlib.pyplot as plt
    import numpy as np
    from scipy.stats import zscore

    # 결측치 확인
    missing_values = df.isnull().sum()

    # Z-score를 사용한 이상치 확인
    df['zscore'] = zscore(df['DataValue'])
    outliers_zscore = df[df['zscore'].abs() > 3]

    # DataValue가 0인 값 확인
    zero_values = df[df['DataValue'] == 0]

    # 전체 시각화
    plt.figure(figsize=(10, 6))
    plt.plot(df['DateTime'], df['DataValue'], marker='o', label='DataValue')
    plt.title('DataValue Trend Over Time')
    plt.xlabel('DateTime')
    plt.ylabel('DataValue')
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    # 각 결과를 한 줄로 출력
    print(f"결측치 개수: {missing_values}")
    print(f"이상치 개수 (Z-score > 3): {outliers_zscore.shape[0]}")
    print(f"DataValue가 0인 값 개수: {zero_values.shape[0]}")

    df = df.drop(columns=['zscore'], errors='ignore')

 # 누락된 데이터포인트 구간을 찾아서 출력
    previous_time = None
    for current_time in df['DateTime']:
        if previous_time is not None:
            time_diff_minutes = (current_time - previous_time).total_seconds() / 60
            if time_diff_minutes > 10:
                missing_points = (time_diff_minutes // 10) - 1
                print(f"누락된 구간: {previous_time.strftime('%Y-%m-%d %H:%M')} ~ {current_time.strftime('%H:%M')}, 누락된 데이터포인트: {int(missing_points)}개")
        previous_time = current_time

analyze_dataframe(df)

"""2024-08-25 08:00 ~ 17:30 분까지의 데이터가 사라졌음을 확인. (인덱스 없음) <br>
08:00 값 정상 /
17:30 0 value /
그 사이: 행 자체가 없음 <br>
이상치와 0value가 동일함을 확인. (7개)

"""

def fill_missing_rows(df):
    # 데이터프레임을 datetime 기준으로 정렬
    df = df.sort_values(by='DateTime').reset_index(drop=True)

    # 시작 시간과 끝 시간을 설정
    start_time = df['DateTime'].min()
    end_time = df['DateTime'].max()

    # 10분 간격으로 모든 시간을 생성
    full_time_range = pd.date_range(start=start_time, end=end_time, freq='10T')

    # 새로운 행을 담을 리스트 생성
    new_rows = []

    for time in full_time_range:
        if not ((df['DateTime'] == time).any()):
            # 10분 단위로 존재하지 않는 시간에는 새로운 행 추가
            new_row = {
                'DataValue': 0,
                'DateTime': time
            }
            new_rows.append(new_row)

    # 새로운 행들을 데이터프레임으로 변환
    new_rows_df = pd.DataFrame(new_rows)

    # 원래의 데이터프레임과 새로 생성한 행들을 합침
    df_full = pd.concat([df, new_rows_df]).sort_values(by='DateTime').reset_index(drop=True)

    return df_full

"""비어있던 구간이 맞게 채워졌는지 확인"""

df_filled = fill_missing_rows(df)
analyze_dataframe(df_filled)

"""결측치 및 이상치 모두 값 채워넣어주기 (선형보간법 사용) <br>
최종 DataSet모양 관찰
"""

df = df_filled
# DataValue가 0인 부분을 선형보간법으로 채우기
df['DataValue'] = df['DataValue'].replace(0, np.nan)  # 0값을 NaN으로 변환
df['DataValue'] = df['DataValue'].interpolate(method='linear')  # 선형 보간법으로 채우기
df['DataValue'] = df['DataValue'].astype(int)

# 결과 출력
analyze_dataframe(df)

"""편의성을 위한 설정. (zscore column 삭제 및 datetime으로 index설정)"""

# zscore column이 있다면 삭제
df = df.drop(columns=['zscore'], errors='ignore')

# DateTime을 index로 설정
df.set_index('DateTime', inplace=True)

"""## 단위 변환 및 각 시간별 유효전력으로 계산"""

# 시간당 유효전력 계산 및 단위 KWH -> MWH로 변경
df['EffectivePower'] = (df['DataValue'] - df['DataValue'].shift(144)) / 1000

# 처음 24시간 동안의 데이터는 NaN이므로 이를 제거
df = df.dropna(subset=['EffectivePower'])

plt.figure(figsize=(12, 6))
plt.plot(df.index, df['EffectivePower'])
plt.title('Effective Power Over Time')
plt.xlabel('DateTime')
plt.ylabel('Effective Power (MWh)')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df.head()

"""파일 저장하기 (로컬 다운로드)"""

from IPython.display import Javascript, display
from google.colab import files, output

# 사용자에게 실행 여부를 묻는 JavaScript 팝업 띄우기
def ask_to_save_file():
    display(Javascript('''
        if (confirm("파일을 저장하시겠습니까?")) {
            google.colab.kernel.invokeFunction('notebook.save_file', [], {});
        } else {
            alert("파일 저장이 취소되었습니다.");
        }
    '''))

# Python 콜백 함수: JavaScript에서 호출되어 파일 저장
def save_file():
    try:
        # 파일명 생성
        start_date = df.index.min().strftime('%Y-%m-%d_%H:%M')
        end_date = df.index.max().strftime('%Y-%m-%d_%H:%M')
        filename = f'{start_date}~{end_date}_inha_activePower_processed.csv'

        # 파일 저장
        df.to_csv(filename)
        files.download(filename)

        # 파일명 출력
        print(f"CSV 파일 저장됨: {filename}")
    except Exception as e:
        print(f"파일 저장 중 오류가 발생했습니다: {str(e)}")

# Colab에서 콜백 함수 등록
output.register_callback('notebook.save_file', save_file)

# 팝업 실행
ask_to_save_file()

"""# 모델 구축 및 예측 수행"""

# 시드 값 고정
seed = 42

# Python 내장 해시 함수의 시드를 고정하여 재현성을 확보 (Python 3.3 이상에서만 적용)
os.environ['PYTHONHASHSEED'] = str(seed)

random.seed(seed) # Python의 random 모듈
np.random.seed(seed )# NumPy의 난수 생성기
torch.manual_seed(seed) # PyTorch의 난수 생성기

torch.cuda.manual_seed(seed) #GPU의 난수 생성기
torch.backends.cudnn.deterministic = True # CuDNN을 사용할 때, 비결정론적 알고리즘을 사용하지 않도록 설정
torch.backends.cudnn.benchmark = False # CuDNN의 벤치마크 기능을 비활성화하여 매번 동일한 연산 경로가 사용되도록 설정

# GPU가 사용 가능하면 CUDA 장치로 설정, 그렇지 않으면 CPU로 설정
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 설정된 장치(device) 출력
device

# 데이터 로드 및 전처리 (데이터셋 나누기 전에 수행하는 것이 좋음!)
def load_data_from_csv(df, column_name):
    data = df
    data = data[column_name].values.reshape(-1, 1)

    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(data)

    return scaled_data, scaler

data, scaler = load_data_from_csv(df, 'EffectivePower')

# 데이터셋 클래스 정의
class B2EDataset(Dataset):
    def __init__(self, data, window_size):
        self.data = data
        self.window_size = window_size

    def __len__(self):
        return len(self.data) - self.window_size

    def __getitem__(self, idx):
        x = self.data[idx:idx+self.window_size]
        y = self.data[idx+self.window_size]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

# 데이터셋 생성
window_size = 24
dataset = B2EDataset(data, window_size)

# 훈련/검증 데이터 분리 (80% 훈련 데이터, 20% 테스트 데이터)
train_size = int(len(dataset) * 0.80)

# 학습과 테스트 데이터셋을 슬라이싱으로 나누기
train_dataset = Subset(dataset, list(range(train_size)))
test_dataset = Subset(dataset, list(range(train_size, len(dataset))))

# 데이터 로더 생성
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

class LSTMModel(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, num_layers=2, output_size=1, dropout=0.2): # LSTM 전체 모델 정의
        super(LSTMModel, self).__init__() # Moudle(부모 클래스의 생성자 호출
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # nn.LSTM을 통해 LSTM 레이어 정의
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)

        # 출력 크기 조정 (FC레이어 = 선형 변환 레이어 정의)
        self.fc = nn.Linear(hidden_size, output_size)

        # 가중치와 편향을 초기화
        self._initialize_weights()

    def _initialize_weights(self):
        for name, param in self.lstm.named_parameters():
            if 'weight' in name:
                nn.init.xavier_uniform_(param.data)
            elif 'bias' in name:
                nn.init.zeros_(param.data)
        nn.init.xavier_uniform_(self.fc.weight)
        nn.init.zeros_(self.fc.bias)

    def forward(self, x): # x는 입력 데이터 (하나의 batch)
        # 초기 은닉 상태와 셀 상태 정의
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)

        # LSTM 통과
        out, _ = self.lstm(x, (h0, c0))

        # 마지막 시퀀스의 출력만 선택하여 FC 레이어에 통과
        out = self.fc(out[:, -1, :])
        out = torch.sigmoid(out)  # 시그모이드 활성화 함수 추가

        return out

# 모델 학습 함수
def train_model(train_loader, num_epochs=30, learning_rate=0.001, device=device):
    model = LSTMModel().to(device)
    criterion = nn.MSELoss() #손실 함수
    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #최적 가중치를 찾아주는 알고리즘

    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0

        for seqs, targets in train_loader:
            seqs, targets = seqs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(seqs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        if (epoch + 1) % 10 == 0:
          avg_loss = epoch_loss / len(train_loader)
          print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')



    return model

# 모델 학습
model = train_model(train_loader, device=device)

# 예측 함수
def predict(model, test_loader, scaler, device=device):
    model.eval()
    predictions = []
    actuals = []

    with torch.no_grad():
        for seqs, targets in test_loader:
            seqs, targets = seqs.to(device), targets.to(device)
            outputs = model(seqs)
            predictions.extend(outputs.cpu().numpy())
            actuals.extend(targets.cpu().numpy())

    predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))
    actuals = scaler.inverse_transform(np.array(actuals).reshape(-1, 1))

    return predictions, actuals

predictions, actuals = predict(model, test_loader, scaler, device=device)

"""## 예측 결과 시각화"""

split_point = int(len(dataset) * 0.8)

train_df = df.iloc[:split_point]  # 훈련 데이터
test_df = df.iloc[split_point:]  # 테스트 데이터 (여기서 index는 DateTime)

# 훈련 및 예측 값을 DataFrame에 저장
predictions_df = pd.DataFrame(predictions, index=test_df.index[:len(predictions)], columns=['Predicted'])
train_predictions, _ = predict(model, train_loader, scaler, device=device)
train_predictions_df = pd.DataFrame(train_predictions, index=train_df.index[:len(train_predictions)], columns=['Predicted'])

# 전체 데이터 그래프
plt.figure(figsize=(15, 6))
plt.plot(df.index, df['EffectivePower'], label='Actual', color='blue')
plt.plot(train_predictions_df.index, train_predictions_df['Predicted'], label='Predicted (Training)', color='orange')
plt.plot(predictions_df.index, predictions_df['Predicted'], label='Predicted (Testing)', color='green')

# 훈련/테스트 데이터셋 구분
plt.axvline(df.index[split_point], color='red', linestyle='--', label='Train/Test Split')

plt.title('Actual vs Predicted Effective Power (Entire Dataset)')
plt.xlabel('DateTime')
plt.ylabel('Effective Power (MWh)')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# 테스트 데이터셋 부분 시각화
plt.figure(figsize=(15, 6))
plt.plot(test_df.index, test_df['EffectivePower'], label='Actual', color='blue')
plt.plot(predictions_df.index, predictions_df['Predicted'], label='Predicted', color='orange')

plt.title('Actual vs Predicted Effective Power (Test Dataset)')
plt.xlabel('DateTime')
plt.ylabel('Effective Power (MWh)')
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# 성능 지표 계산
mae = mean_absolute_error(actuals, predictions)
rmse = mean_squared_error(actuals, predictions, squared=False)
r2 = r2_score(actuals, predictions)

# 성능 지표 출력
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared (R2): {r2:.4f}")

"""# 날짜 와 데이터포인트를 넘겨 이후 예측하기


"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import torch
from torch.utils.data import DataLoader, Subset

def load_data_from_df(df, column_name):
    """ 데이터 스케일링 함수 """
    data = df[column_name].values.reshape(-1, 1)

    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(data)

    return scaled_data, scaler

def generate_predictions(df, model, cutoff_date, num_predictions, window_size, device):
    """
    특정 기간까지 학습 후, 그 이후로 주어진 수만큼 데이터를 예측하는 함수

    Parameters:
    df : pd.DataFrame - 전체 데이터프레임
    model : LSTMModel - 학습된 모델
    cutoff_date : str - 학습을 종료할 날짜 및 시각 (예: '2024-09-27 04:50:00')
    num_predictions : int - 예측할 데이터 포인트 수
    window_size : int - 입력으로 사용할 과거 데이터의 시점 수
    device : torch.device - 학습 및 예측에 사용할 디바이스 (GPU/CPU)

    Returns:
    pd.DataFrame - 예측된 값이 포함된 데이터프레임
    """

    # 1. 학습할 데이터와 예측할 데이터 나누기
    cutoff_datetime = pd.to_datetime(cutoff_date)
    train_df = df[df.index < cutoff_datetime]
    test_df = df[df.index >= cutoff_datetime]

    # 2. 데이터 전처리 및 스케일링
    data, scaler = load_data_from_df(train_df, 'EffectivePower')

    # 3. 데이터셋 생성 및 분리
    dataset = B2EDataset(data, window_size)
    train_size = int(len(dataset) * 0.8)
    train_dataset = Subset(dataset, list(range(train_size)))
    test_dataset = Subset(dataset, list(range(train_size, len(dataset))))

    # 4. 데이터 로더 생성
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

    # 5. 모델 학습
    model = train_model(train_loader, device=device)

    # 6. 예측 (num_predictions 만큼)
    prediction_input = data[-window_size:]  # 마지막 window_size 만큼의 데이터를 입력으로 사용
    predictions_list = []

    for _ in range(num_predictions):
        input_tensor = torch.tensor(prediction_input, dtype=torch.float32).unsqueeze(0).to(device)
        prediction = model(input_tensor)
        prediction_np = prediction.cpu().detach().numpy()[0][0]

        predictions_list.append(prediction_np)

        # 다음 예측을 위해 이전 예측 값을 추가
        prediction_input = np.append(prediction_input[1:], [[prediction_np]], axis=0)

    # 7. 스케일 역변환
    predictions_scaled = scaler.inverse_transform(np.array(predictions_list).reshape(-1, 1))

    # 8. 예측 값 데이터프레임으로 변환
    test_df_predictions = pd.DataFrame(predictions_scaled,
                                       index=pd.date_range(start=test_df.index[0],
                                                           periods=num_predictions, freq='10T'),
                                       columns=['Predicted'])

    return train_df, test_df_predictions


def plot_predictions(train_df, test_df_predictions):
    """
    실제 값과 예측 값을 시각화하는 함수

    Parameters:
    train_df : pd.DataFrame - 학습에 사용된 실제 데이터
    test_df_predictions : pd.DataFrame - 예측된 데이터프레임
    """
    plt.figure(figsize=(15, 6))
    plt.plot(train_df.index, train_df['EffectivePower'], label='Actual (Training)', color='blue')
    plt.plot(test_df_predictions.index, test_df_predictions['Predicted'], label='Predicted', color='orange')

    plt.title('Actual vs Predicted Effective Power')
    plt.xlabel('DateTime')
    plt.ylabel('Effective Power (MWh)')
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

"""### DB에서 실제 값 불러오기"""

import pandas as pd

def get_energy_data(start_date, end_date):
    # SQL 쿼리 작성
    query = f"""
        SELECT
            DateTime,
            DataValue,
            (DataValue - LAG(DataValue, 144) OVER (ORDER BY DateTime)) / 1000 AS MWH
        FROM
            Tech_All_KWH
        WHERE
            Building = 
            AND DateTime BETWEEN '{start_date}' AND '{end_date}'  -- 날짜 범위를 추가
        ORDER BY
            DateTime
    """

    # 쿼리 실행
    cursor.execute(query)
    rows = cursor.fetchall()

    # 쿼리 결과 확인
    print("쿼리 결과 확인 (상위 5개):", rows[:5])

    # DataFrame으로 변환
    df = pd.DataFrame([list(row) for row in rows], columns=[ 'DateTime','DataValue', 'MVH'])

    # 'DateTime' 열을 datetime 형식으로 변환
    df['DateTime'] = pd.to_datetime(df['DateTime'])

    # 'DateTime'을 인덱스로 설정 (단일 인덱스)
    df.set_index('DateTime', inplace=True)

    return df

# 함수 사용 예시
cutoff_date = '2024-09-27 10:00:00'  # 학습 데이터 종료 시점
num_predictions = 48  # 예측할 데이터 개수
window_size = 24  # LSTM 모델에 입력할 시점 수
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# train_df와 test_df_predictions 생성
train_df, test_df_predictions = generate_predictions(df, model, cutoff_date, num_predictions, window_size, device)

# 예측 값 시각화
plot_predictions(train_df, test_df_predictions)

import pandas as pd
from datetime import timedelta

# cutoff_date와 num_predictions 값을 사용하여 start_date와 end_date 계산
def calculate_dates(cutoff_date, num_predictions):
    cutoff_datetime = pd.to_datetime(cutoff_date)

    # cutoff_date로부터 하루 전이 start_date
    start_date = cutoff_datetime - timedelta(days=1)

    # cutoff_date로부터 num_predictions * 10분 후가 end_date
    end_date = cutoff_datetime + timedelta(minutes=num_predictions * 10)

    return start_date, end_date

# 함수 사용 예시
cutoff_date = '2024-09-27 10:00:00'  # 학습 데이터 종료 시점
num_predictions = 48  # 예측할 데이터 개수

# start_date와 end_date 계산
start_date, end_date = calculate_dates(cutoff_date, num_predictions)

# 결과 출력
print(f"Start Date: {start_date.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"End Date: {end_date.strftime('%Y-%m-%d %H:%M:%S')}")

df_result = get_energy_data(start_date, end_date)
merged_df = pd.merge(test_df_predictions, df_result, left_index=True, right_index=True, how='inner')

"""## 예측 / 실제 값 비교"""

import pandas as pd

def calculate_difference(test_df_predictions, df_result):
    """
    test_df_predictions의 EffectivePower와 df_result의 MWH 값을 비교하고,
    차이를 계산하여 새로운 DataFrame(diff_df)을 생성하는 함수

    Parameters:
    test_df_predictions : pd.DataFrame - 예측된 데이터프레임
    df_result : pd.DataFrame - 실제 MWH 값이 포함된 데이터프레임

    Returns:
    diff_df : pd.DataFrame - 날짜, 예측된 EffectivePower, 실제 MWH, 그리고 차이(diff)가 포함된 데이터프레임


    """

    # 두 DataFrame의 인덱스를 기준으로 병합 (DateTime을 기준으로 병합)
    merged_df = pd.merge(test_df_predictions, df_result, left_index=True, right_index=True, how='inner')


    # 차이 계산 (EffectivePower - MWH)
    merged_df['diff'] = merged_df['Predicted'] - merged_df['MVH']

    # 필요한 컬럼만 선택하여 새로운 DataFrame 생성
    diff_df = merged_df[['Predicted', 'MVH', 'diff']].copy()
    diff_df.columns = ['Predicted', 'Actual', 'Difference']


    return diff_df[[ 'Predicted', 'Actual', 'Difference']]

# 함수 사용 예시
calculate_difference(test_df_predictions, df_result)

import matplotlib.pyplot as plt

def visualize_predictions(diff_df):
    """
    Predicted, Actual 값을 한 그래프에 그리고, Difference 값을 subplot으로 따로 그리는 함수

    Parameters:
    diff_df : pd.DataFrame - Predicted, Actual, Difference 값이 포함된 데이터프레임
    """

    # 그래프 그리기
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)

    # Predicted와 Actual 값을 같은 그래프에 그리기
    ax1.plot(diff_df.index, diff_df['Predicted'], label='Predicted', color='blue', marker='o')
    ax1.plot(diff_df.index, diff_df['Actual'], label='Actual', color='green', marker='x')
    ax1.set_title('Predicted vs Actual')
    ax1.set_ylabel('Value')
    ax1.legend()
    ax1.grid(True)

    # Difference 값을 subplot으로 따로 그리기
    ax2.plot(diff_df.index, diff_df['Difference'], label='Difference', color='red', marker='d')
    ax2.set_title('Difference between Predicted and Actual')
    ax2.set_xlabel('DateTime')
    ax2.set_ylabel('Difference')
    ax2.legend()
    ax2.grid(True)

    # 날짜 라벨을 더 잘 보이게 하기 위해 회전
    plt.xticks(rotation=45)

    # 레이아웃 조정
    plt.tight_layout()

    # 그래프 보여주기
    plt.show()

# 함수 사용 예시
diff_df = calculate_difference(test_df_predictions, df_result)
visualize_predictions(diff_df)

import pandas as pd
from IPython.display import display, HTML

def highlight_large_differences(diff_df):
    """
    Difference가 1 이상인 행을 빨간색 글씨로 출력하는 함수

    Parameters:
    diff_df : pd.DataFrame - Predicted, Actual, Difference 값이 포함된 데이터프레임
    """
    # Difference가 1 이상인 행을 필터링
    large_diff_df = diff_df[diff_df['Difference'] >= 0.5]

    if not large_diff_df.empty:
        # 스타일 적용하여 Difference가 큰 행을 빨간색으로 출력
        display(HTML('<h3>Difference가 0.5 이상인 행</h3>'))
        display(large_diff_df.style.applymap(lambda x: 'color: red' if x >= 0.5 else '', subset=['Difference']))
    else:
        print("Difference가 0.5 이상인 행이 없습니다.")

# 함수 사용 예시
diff_df = calculate_difference(test_df_predictions, df_result)
highlight_large_differences(diff_df)

"""# 서버 연결 종료"""

from IPython.display import Javascript, display
from google.colab import output

# 사용자에게 실행 여부를 묻는 JavaScript 팝업 띄우기
def ask_to_close_cursor():
    display(Javascript('''
        if (confirm("cursor.close()를 실행하시겠습니까?")) {
            google.colab.kernel.invokeFunction('notebook.close_cursor', [], {});
        } else {
            alert("실행이 취소되었습니다.");
        }
    '''))

# Python 콜백 함수: JavaScript에서 호출되어 cursor.close() 실행
def close_cursor():
    try:
        # 실제로 실행할 cursor.close()
        cursor.close()
        print("cursor.close()가 성공적으로 실행되었습니다.")
    except NameError:
        print("cursor가 정의되지 않았습니다. 먼저 cursor를 정의하세요.")

# 콜백 등록
output.register_callback('notebook.close_cursor', close_cursor)

# 팝업 실행
ask_to_close_cursor()

"""# breakpoint"""

raise SystemExit("사용자에 의해 실행 중단")

"""# 추가 개발 사항

## 체크 포인트 생성 -> 모델 이어서 학습하도록 만들기 <br>
CT Pipeline 구축
"""

def save_checkpoint(model, optimizer, epoch, file_path='checkpoint.pth'):
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()
    }
    torch.save(checkpoint, file_path)
    print(f"Checkpoint saved at {file_path}")

def load_checkpoint(model, optimizer, file_path='checkpoint.pth'):
    checkpoint = torch.load(file_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    epoch = checkpoint['epoch']
    print(f"Checkpoint loaded from {file_path}, resuming from epoch {epoch}")
    return epoch

"""## Early Stopping 구현. 검증 데이터 분류 필요"""

# Early Stopping 기준: patience는 검증 손실이 개선되지 않는 에폭 수
def train_model(train_loader, test_loader, num_epochs=30, learning_rate=0.001, device='cuda', checkpoint_interval=10, save_path='checkpoint.pth', patience=5):
    model = LSTMModel().to(device)
    criterion = nn.MSELoss()  # 손실 함수
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # 최적화 알고리즘

    best_loss = float('inf')  # 최소 손실값을 추적
    epochs_no_improve = 0  # Early Stopping을 위한 개선되지 않은 에폭 수

    for epoch in range(num_epochs):
        model.train()
        epoch_loss = 0

        for seqs, targets in train_loader:
            seqs, targets = seqs.to(device), targets.to(device)
            optimizer.zero_grad()
            outputs = model(seqs)
            loss = criterion(outputs, targets)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        # 훈련 데이터셋에서의 평균 손실 계산
        avg_train_loss = epoch_loss / len(train_loader)

        # 검증 데이터셋에서의 손실 계산
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for seqs, targets in val_loader:
                seqs, targets = seqs.to(device), targets.to(device)
                outputs = model(seqs)
                loss = criterion(outputs, targets)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)
        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')

        # Early Stopping 체크
        if avg_val_loss < best_loss:
            best_loss = avg_val_loss
            epochs_no_improve = 0
            save_checkpoint(model, optimizer, epoch+1, file_path=save_path)  # 손실이 개선되면 체크포인트 저장
        else:
            epochs_no_improve += 1

        if epochs_no_improve >= patience:
            print(f"Early stopping at epoch {epoch+1}. No improvement in validation loss for {patience} consecutive epochs.")
            break

    return model

# 모델, 옵티마이저 정의
model = LSTMModel().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 체크포인트에서 불러오기
start_epoch = load_checkpoint(model, optimizer, file_path='checkpoint.pth')

# 이후 이어서 학습
train_model(train_loader, num_epochs=30, learning_rate=0.001, device=device, checkpoint_interval=10, save_path='checkpoint.pth')

"""## git 연동"""

!apt-get install git

# 깃허브 사용자 정보 설정 (이메일과 이름)
!git config --global user.email "hi653@naver.com"
!git config --global user.name "codrae"